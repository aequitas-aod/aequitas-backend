- id: { code: DatasetSelection }
  text: Choose a dataset or load your own.
  type: single
  answers:
  - id: { code: Custom }
    text: Custom
    description: Load your own dataset.
    content: |
      Custom:
        description: Load your own dataset.
  - id: { code: AdultDataset }
    text: Adult Census Income Dataset
    description: null
    content: |
      AdultDataset:
        description: Adult Census Income Dataset.
  - id: { code: BankDataset }
    text: Bank Marketing Dataset
    description: null
    content: |
      BankDataset:
        description: Bank marketing Dataset.
  - id: { code: CompasDataset }
    text: ProPublica COMPAS Dataset
    description: null
    content: |
      CompasDataset:
        description: ProPublica COMPAS Dataset.
  - id: { code: GermanDataset }
    text: German credit Dataset
    description: null
    content: |
      GermanDataset:
        description: German credit Dataset.
  - id: { code: LawSchoolGPADataset }
    text: Law School GPA Dataset
    description: null
    content: |
      LawSchoolGPADataset:
        description: Law School GPA dataset.
  - id: { code: MEPSDataset19 }
    text: Medical Expenditure Panel Survey Dataset 2019
    description: null
    content: |
      MEPSDataset19:
        description: Medical Expenditure Panel Survey Dataset 2019.
  - id: { code: MEPSDataset20 }
    text: Medical Expenditure Panel Survey Dataset 2020
    description: null
    content: |
      MEPSDataset20:
        description: Medical Expenditure Panel Survey Dataset 2020.
  - id: { code: MEPSDataset21 }
    text: Medical Expenditure Panel Survey Dataset 2021
    description: null
    content: |
      MEPSDataset21:
        description: Medical Expenditure Panel Survey Dataset 2021.
  enabled_by: []
  action_needed: null
  created_at: 2024-05-31T18:20:23

- id: { code: OutputFeatures }
  text: Which are the output features?
  type: multiple
  answers:
    - id: { code: OutputFeatures-features-selected }
      text: ""
      description: null
  enabled_by:
    - { code: Custom }
    - { code: AdultDataset }
    - { code: BankDataset }
    - { code: CompasDataset }
    - { code: GermanDataset }
    - { code: LawSchoolGPADataset }
    - { code: MEPSDataset19 }
    - { code: MEPSDataset20 }
    - { code: MEPSDataset21 }
  action_needed: null
  created_at: 2024-05-31T18:20:24

# in the project question it will be:
#  content: |
#    FeatureSelection:
#      features:
#        - Age
#        - Workclass
#        - Education
#        - Occupation

- id: { code: SensitiveFeatures }
  text: Which are the sensitive features?
  type: multiple
  answers:
    - id: { code: SensitiveFeatures-features-selected }
      text: ""
      description: null
  enabled_by:
    - { code: OutputFeatures-features-selected }
  action_needed: null
  created_at: 2024-05-31T18:20:25

# in project question, same content of OutputFeatures question

- id: { code: Proxies }
  text: Specify the proxies for the sensitive features.
  type: multiple
  answers:
    - id: { code: Proxies-features-selected }
      text: ""
      description: null
  enabled_by:
    - { code: SensitiveFeatures-features-selected }
  action_needed: null
  created_at: 2024-05-31T18:20:26

- id: { code: Detection }
  text: Select the fairness metrics and the features to check.
  type: multiple
  answers:
    - id: { code: StatisticalParity }
      text: Statistical Parity
      description: Statistical Parity Difference (SPD) measures the difference between the probability that a model assigns a positive outcome to a protected group versus an unprotected group, we got fairness when the measure equals 0.
      content: |
        StatisticalParity:
          name: Statistical Parity
          description: Difference between the probability of a favorable outcome for the unprivileged group and the probability of a favorable outcome for the privileged group.
    - id: { code: DisparateImpact }
      text: Disparate Impact
      description: Disparate Impact (DI) examines the ratio of favorable outcomes for two groups â€” a majority and a minority. A fair assessment is achieved when this measure equals 1.
      content: |
        DisparateImpact:
          name: Disparate Impact
          description: Ratio of the probability of a favorable outcome for the unprivileged group to the probability of a favorable outcome for the privileged group.
    - id: { code: EqualOpportunity }
      text: Equal Opportunity
      description: The Equal Opportunity Difference (EOD) measures the extent to which a model deviates from providing equal opportunity, meaning that an equal proportion of each group receives a favorable outcome. For fairness, this metric should be 0.
      content: |
        EqualOpportunity:
          name: Equal Opportunity
          description: Difference between the true positive rate for the unprivileged group and the true positive rate for the privileged group.
    - id: { code: AverageOdds }
      text: Average Absolute Odds
      description: The Average Absolute Odds Difference (AAOD) is a metric used to assess bias in predictive models by analyzing the difference between the false positive rate and the true positive rate across various groups or populations. For a model to be considered fair, this measure should be 0.
      content: |
        AverageOdds:
          name: Average Odds
          description: Average of the true positive rate difference and false positive rate difference between the unprivileged and privileged groups.
  enabled_by:
    - { code: Proxies-features-selected }
  action_needed: null
  created_at: 2024-05-31T18:20:27

- id: { code: DataMitigation }
  text: Which data mitigation technique do you want to apply?
  type: single
  answers:
    - id: { code: DisparateImpactRemover }
      text: Disparate Impact Remover
      description: Disparate Impact Remover is a preprocessing technique that edits feature values increase group fairness while preserving rank-ordering within groups.
    - id: { code: LearningFairRepresentations }
      text: Learning Fair Representations
      description: Learning Fair Representations (LFR) aim to find and create a latent representation that encodes the data well but obfuscates information about protected attributes.
    - id: { code: OptimizedPreprocessing }
      text: Optimized Preprocessing
      description: The Optimized Pre-Processing method applies a probabilistic transformation to modify features and labels in the data, ensuring that group fairness is maintained while minimizing individual distortion and preserving data fidelity.
    - id: { code: Reweighing }
      text: Reweighing
      description: The Reweighing method assigns different weights to training samples for each group and label combination, aiming to ensure fairness before classification.
  enabled_by:
    - { code: StatisticalParity }
    - { code: DisparateImpact }
    - { code: EqualOpportunity }
    - { code: AverageOdds }
    - { code: MitigateDataAgain } # note the comeback from the summary
  action_needed: null
  created_at: 2024-05-31T18:20:28

- id: { code: DataMitigationSummary }
  text: What do you want to do next?
  type: single
  answers:
    - id: { code: MitigateDataAgain}
      text: Mitigate Data Again
      description: null
    - id: { code: MitigateModel }
      text: Mitigate Model
      description: null
    - id: { code: Done }
      text: Done
      description: null
  enabled_by:
    - { code: DisparateImpactRemover }
    - { code: LearningFairRepresentations }
    - { code: OptimizedPreprocessing }
    - { code: Reweighing }
  action_needed: null
  created_at: 2024-05-31T18:20:29

- id: { code: ModelMitigation }
  text: Which model mitigation technique do you want to apply?
  type: single
  answers:
    - id: { code: AdversarialDebiasing }
      text: Adversarial Debiasing
      description: Adversarial Debiasing is based on the idea of training one main model (called the classifier), while at the same time training a second model (called the adversary). The goal of the adversary model is to try to predict the sensitive feature (e.g., race or gender) from the classifier's output.
    - id: { code: PrejudiceRemover }
      text: Prejudice Remover
      description: Prejudice Remover is a method that modifies the objective function of the classifier by inserting a term that penalizes discrimination based on a sensitive feature. This term is added to the loss function of the model, and its weight can be controlled by a parameter that defines the degree of penalty
    - id: { code: MetaFairClassifier }
      text: MetaFair Classifier
      description: Meta-Fair Classifier is algorithm that uses an optimization method based on surrogate loss functions to ensure fairness. The idea is to train a classifier that meets specific equity metrics defined during training. This is done by incorporating a surrogate loss function that takes equity metrics into account in the optimization process. Equity metrics are translated into optimization constraints during model training.
  enabled_by:
    - { code: MitigateModel }
    - { code: MitigateModelAgain } # note the comeback from the summary
  action_needed: null
  created_at: 2024-05-31T18:20:30

- id: { code: ModelMitigationSummary }
  text: What do you want to do next?
  type: single
  answers:
    - id: { code: MitigateDataAgain }
      text: Mitigate Data Again
      description: null
    - id: { code: MitigateModelAgain }
      text: Mitigate Model Again
      description: null
    - id: { code: MitigateOutcome }
      text: Mitigate Outcome
      description: null
    - id: { code: Done }
      text: Done
      description: null
  enabled_by:
    - { code: AdversarialDebiasing }
    - { code: GerryFairClassifier }
    - { code: MetaFairClassifier }
    - { code: PrejudiceRemover }
  action_needed: null
  created_at: 2024-05-31T18:20:31

- id: { code: OutcomeMitigation }
  text: Which outcome mitigation technique do you want to apply?
  type: single
  answers:
    - id: { code: EqualizedOdds }
      text: Equalized Odds
      description: Equalized Odds is a post-processing algorithm designed to correct bias in the results of a machine learning model, ensuring that false positive rates (FPR) and false negative rates (FNR) are equal among different sensitive groups (e.g., groups based on race, gender, ethnicity). The main goal is to achieve fairness in the results predicted by the model, regardless of membership in a specific group.
    - id: { code: CalibratedEqualizedOdds }
      text: Calibrated Equalized Odds
      description: The Calibrated Equalized Odds algorithm is very similar to the Equalized Odds algorithm by adding a calibration component. It therefore respects 2 key concepts. Equalized Odds, the distribution of errors (false positives and false negatives) should be equal for all groups; Calibration, the predicted probabilities should correctly reflect the reality for each group.
    - id: { code: RejectOptionClassification }
      text: Reject Option Classification
      description: Reject Option Classification (ROC) is a post-processing algorithm used to improve fairness in machine learning models by intervening in areas where the model is less confident in its predictions.
  enabled_by:
    - { code: MitigateOutcome }
    - { code: MitigateOutcomeAgain }
  action_needed: null
  created_at: 2024-05-31T18:20:32

- id: { code: OutcomeMitigationSummary }
  text: What do you want to do next?
  type: single
  answers:
    - id: { code: MitigateDataAgain}
      text: Mitigate Data Again
      description: null
    - id: { code: MitigateModelAgain }
      text: Mitigate Model Again
      description: null
    - id: { code: MitigateOutcomeAgain }
      text: Mitigate Outcome Again
      description: null
    - id: { code: Test }
      text: Test
      description: null
    - id: { code: Done }
      text: Done
      description: null
  enabled_by:
    - { code: CalibratedEqualizedOdds }
    - { code: EqualizedOdds }
    - { code: RejectOptionClassification }
  action_needed: null
  created_at: 2024-05-31T18:20:33

- id: { code: TestSetChoice }
  text: Choose a test set.
  type: single
  answers:
    - id: { code: TestSet1 }
      text: Test Set 1
      description: null
    - id: { code: TestSet2 }
      text: Test Set 2
      description: null
    - id: { code: TestSet3 }
      text: Test Set 3
      description: null
  enabled_by:
    - { code: Test }
  action_needed: null
  created_at: 2024-05-31T18:20:34

- id: { code: Polarization }
  text: Select the polarization metrics and the features to check.
  type: multiple
  answers:
    - id: { code: DemographicParity }
      text: Demographic Parity
      description: null
      content: |
        DemographicParity:
          name: Demographic Parity
          description: Difference between the probability of a favorable outcome for the unprivileged group and the probability of a favorable outcome for the privileged group.
    - id: { code: EqualizedOdds }
      text: Equalized Odds
      description: null
      content: |
        EqualizedOdds:
          name: Equalized Odds
          description: Difference between the true positive rate for the unprivileged group and the true positive rate for the privileged group.

  enabled_by:
    - { code: TestSet1 }
    - { code: TestSet2 }
    - { code: TestSet3 }
  action_needed: null
  created_at: 2024-05-31T18:20:35

- id: { code: TestSummary }
  text: What do you want to do next?
  type: single
  answers:
    - id: { code: MitigateDataAgain }
      text: Mitigate Data Again
      description: null
    - id: { code: MitigateModelAgain }
      text: Mitigate Model Again
      description: null
    - id: { code: MitigateOutcomeAgain }
      text: Mitigate Outcome Again
      description: null
    - id: { code: Done }
      text: Done
      description: null
  enabled_by:
    - { code: DemographicParity }
    - { code: EqualizedOdds }
  action_needed: null
  created_at: 2024-05-31T18:20:36