- id: { code: DatasetSelection }
  text: Choose a dataset or load your own.
  description: Choose or provide a dataset which will be subject to the fairness process.
  type: single
  answers:
    - id: { code: Custom }
      text: Custom
      description: Load your own dataset.
    - id: { code: AdultDataset }
      text: Adult Census Income Dataset
      description: null
    - id: { code: CompasDataset }
      text: ProPublica COMPAS Dataset
      description: null
    - id: { code: GermanDataset }
      text: German credit Dataset
      description: null
  enabled_by: []
  action_needed: null
  created_at: 2024-05-31T18:20:23

- id: { code: OutputFeatures }
  text: Which are the output features?
  description: Mark the features that will be used as the output of the model.
  type: multiple
  answers:
    - id: { code: OutputFeatures-features-selected }
      text: ""
      description: null
  enabled_by:
    - { code: Custom }
    - { code: AdultDataset }
    - { code: BankDataset }
    - { code: CompasDataset }
    - { code: GermanDataset }
    - { code: LawSchoolGPADataset }
    - { code: MEPSDataset19 }
    - { code: MEPSDataset20 }
    - { code: MEPSDataset21 }
  action_needed: null
  created_at: 2024-05-31T18:20:24

- id: { code: SensitiveFeatures }
  text: Which are the sensitive features?
  description: null
  type: multiple
  answers:
    - id: { code: SensitiveFeatures-features-selected }
      text: ""
      description: null
  enabled_by:
    - { code: OutputFeatures-features-selected }
  action_needed: null
  created_at: 2024-05-31T18:20:25

- id: { code: Proxies }
  text: Specify the proxies for the sensitive features.
  description: Proxies are features that are correlated with the sensitive features but are not themselves sensitive.
  type: multiple
  answers:
    - id: { code: Proxies-features-selected }
      text: ""
      description: null
  enabled_by:
    - { code: SensitiveFeatures-features-selected }
  action_needed: null
  created_at: 2024-05-31T18:20:26

- id: { code: Detection }
  text: Select the fairness metrics and the features to check.
  description: The selected metrics will be mitigated in the next steps.
  type: multiple
  answers:
    - id: { code: StatisticalParity }
      text: Statistical Parity
      description: Statistical Parity Difference (SPD) measures the difference between the probability that a model assigns a positive outcome to a protected group versus an unprotected group, we got fairness when the measure equals 0.
    - id: { code: DisparateImpact }
      text: Disparate Impact
      description: Disparate Impact (DI) examines the ratio of favorable outcomes for two groups â€” a majority and a minority. A fair assessment is achieved when this measure equals 1.
  enabled_by:
    - { code: Proxies-features-selected }
  action_needed: null
  created_at: 2024-05-31T18:20:27

- id: { code: DataMitigation }
  text: Which data mitigation technique do you want to apply?
  description: null
  type: single
  answers:
    - id: { code: DisparateImpactRemover }
      text: Disparate Impact Remover
      description: Disparate Impact Remover is a preprocessing technique that edits feature values increase group fairness while preserving rank-ordering within groups.
    - id: { code: LearningFairRepresentations }
      text: Learning Fair Representations
      description: Learning Fair Representations (LFR) aim to find and create a latent representation that encodes the data well but obfuscates information about protected attributes.
    - id: { code: Reweighing }
      text: Reweighing
      description: The Reweighing method assigns different weights to training samples for each group and label combination, aiming to ensure fairness before classification.
    - id: { code: CorrelationRemover }
      text: Correlation Remover
      description: The Correlation Remover applies a linear transformation to the non-sensitive feature columns in order to remove their correlation with the sensitive feature columns while retaining as much information as possible (as measured by the least-squares error).
    - id: { code: NoDataMitigation }
      text: Do Not Mitigate
      description: null
  enabled_by:
    - { code: StatisticalParity }
    - { code: DisparateImpact } # note the comeback from the summary
  action_needed: null
  created_at: 2024-05-31T18:20:28

- id: { code: DataMitigationSummary }
  text: What do you want to do next?
  description: null
  type: single
  answers:
    - id: { code: MitigateDataAgain}
      text: Mitigate Data Again
      description: null
    - id: { code: MitigateModel }
      text: Mitigate Model
      description: null
    - id: { code: Done }
      text: Done
      description: null
    - id: { code: Test }
      text: Validate on Test Data
      description: null
  enabled_by:
    - { code: DisparateImpactRemover }
    - { code: LearningFairRepresentations }
    - { code: Reweighing }
    - { code: CorrelationRemover }
  action_needed: null
  created_at: 2024-05-31T18:20:29

- id: { code: ModelMitigation }
  text: Which model mitigation technique do you want to apply?
  description: null
  type: single
  answers:
    - id: { code: AdversarialDebiasing }
      text: Adversarial Debiasing
      description: Adversarial Debiasing is based on the idea of training one main model (called the classifier), while at the same time training a second model (called the adversary). The goal of the adversary model is to try to predict the sensitive feature (e.g., race or gender) from the classifier's output.
    - id: { code: PrejudiceRemover }
      text: Prejudice Remover
      description: Prejudice Remover is a method that modifies the objective function of the classifier by inserting a term that penalizes discrimination based on a sensitive feature. This term is added to the loss function of the model, and its weight can be controlled by a parameter that defines the degree of penalty.
    - id: { code: MetaFairClassifier }
      text: MetaFair Classifier
      description: Meta-Fair Classifier is algorithm that uses an optimization method based on surrogate loss functions to ensure fairness. The idea is to train a classifier that meets specific equity metrics defined during training. This is done by incorporating a surrogate loss function that takes equity metrics into account in the optimization process. Equity metrics are translated into optimization constraints during model training.
    - id: { code: NoModelMitigation }
      text: Do Not Mitigate
      description: null  
  enabled_by:
    - { code: MitigateModel }
    - { code: MitigateModelAgain }
    - { code: NoDataMitigation } # note the comeback from the summary
  action_needed: null
  created_at: 2024-05-31T18:20:30

- id: { code: ModelMitigationSummary }
  text: What do you want to do next?
  description: null
  type: single
  answers:
    - id: { code: MitigateDataAgain }
      text: Mitigate Data Again
      description: null
    - id: { code: MitigateModelAgain }
      text: Mitigate Model Again
      description: null
    - id: { code: MitigateOutcome }
      text: Mitigate Outcome
      description: null
    - id: { code: Test }
      text: Validate on Test Data
      description: null
    - id: { code: Done }
      text: Done
      description: null
  enabled_by:
    - { code: AdversarialDebiasing }
    - { code: PrejudiceRemover }
    - { code: MetaFairClassifier }
  action_needed: null
  created_at: 2024-05-31T18:20:31

- id: { code: OutcomeMitigation }
  text: Which outcome mitigation technique do you want to apply?
  description: null
  type: single
  answers:
    - id: { code: EqualizedOdds }
      text: Equalized Odds
      description: Equalized Odds is a post-processing algorithm designed to correct bias in the results of a machine learning model, ensuring that false positive rates (FPR) and false negative rates (FNR) are equal among different sensitive groups (e.g., groups based on race, gender, ethnicity). The main goal is to achieve fairness in the results predicted by the model, regardless of membership in a specific group.
    - id: { code: CalibratedEqualizedOdds }
      text: Calibrated Equalized Odds
      description: The Calibrated Equalized Odds algorithm is very similar to the Equalized Odds algorithm by adding a calibration component. It therefore respects 2 key concepts. Equalized Odds, the distribution of errors (false positives and false negatives) should be equal for all groups; Calibration, the predicted probabilities should correctly reflect the reality for each group.
    - id: { code: RejectOptionClassification }
      text: Reject Option Classification
      description: Reject Option Classification (ROC) is a post-processing algorithm used to improve fairness in machine learning models by intervening in areas where the model is less confident in its predictions.
    - id: { code: NoOutcomeMitigation }
      text: Do Not Mitigate
      description: null
  enabled_by:
    - { code: MitigateOutcome }
    - { code: MitigateOutcomeAgain }
    - { code: NoModelMitigation }
  action_needed: null
  created_at: 2024-05-31T18:20:32

- id: { code: OutcomeMitigationSummary }
  text: What do you want to do next?
  description: null
  type: single
  answers:
    - id: { code: MitigateDataAgain}
      text: Mitigate Data Again
      description: null
    - id: { code: MitigateModelAgain }
      text: Mitigate Model Again
      description: null
    - id: { code: MitigateOutcomeAgain }
      text: Mitigate Outcome Again
      description: null
    - id: { code: Test }
      text: Validate on Test Data
      description: null
    - id: { code: Done }
      text: Done
      description: null
  enabled_by:
    - { code: CalibratedEqualizedOdds }
    - { code: EqualizedOdds }
    - { code: RejectOptionClassification }
  action_needed: null
  created_at: 2024-05-31T18:20:33

- id: { code: TestSetChoice }
  text: Choose the Test Data according to your Training Data.
  description: null
  type: single
  answers:
    - id: { code: Test-Custom }
      text: Custom
      description: Load your own dataset.
    - id: { code: Test-AdultDataset }
      text: Adult Census Income Dataset
      description: null
    - id: { code: Test-BankDataset }
      text: Bank Marketing Dataset
      description: null
    - id: { code: Test-CompasDataset }
      text: ProPublica COMPAS Dataset
      description: null
    - id: { code: Test-GermanDataset }
      text: German credit Dataset
      description: null
    - id: { code: Test-LawSchoolGPADataset }
      text: Law School GPA Dataset
      description: null
    - id: { code: Test-MEPSDataset19 }
      text: Medical Expenditure Panel Survey Dataset 2019
      description: null
    - id: { code: Test-MEPSDataset20 }
      text: Medical Expenditure Panel Survey Dataset 2020
      description: null
    - id: { code: Test-MEPSDataset21 }
      text: Medical Expenditure Panel Survey Dataset 2021
      description: null
  enabled_by:
    - { code: Test }
    - { code: TestAgain }
  action_needed: null
  created_at: 2024-05-31T18:20:34

- id: { code: Polarization }
  text: Select the polarization algorithm according to your data.
  description: null
  type: multiple
  answers:
    - id: { code: CategoricalPolarization }
      text: Categorical Polarization
      description: null
    - id: { code: NumericalPolarization }
      text: Numerical Polarization
      description: null
  enabled_by:
    - { code: Test-Custom }
    - { code: Test-AdultDataset }
    - { code: Test-BankDataset }
    - { code: Test-CompasDataset }
    - { code: Test-GermanDataset }
    - { code: Test-LawSchoolGPADataset }
    - { code: Test-MEPSDataset19 }
    - { code: Test-MEPSDataset20 }
    - { code: Test-MEPSDataset21 }
  action_needed: null
  created_at: 2024-05-31T18:20:35

- id: { code: TestSummary }
  text: What do you want to do next?
  description: null
  type: single
  answers:
    - id: { code: MitigateDataAgain }
      text: Mitigate Data Again
      description: null
    - id: { code: MitigateModelAgain }
      text: Mitigate Model Again
      description: null
    - id: { code: MitigateOutcomeAgain }
      text: Mitigate Outcome Again
      description: null
    - id: { code: TestAgain }
      text: Validate on Test Data Again
      description: null
    - id: { code: Done }
      text: Done
      description: null
  enabled_by:
    - { code: CategoricalPolarization }
    - { code: NumericalPolarization }
  action_needed: null
  created_at: 2024-05-31T18:20:36