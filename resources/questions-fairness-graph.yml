- id: { code: DatasetTypeSelection }
  text: Which type of dataset are you going to use?
  description: Choose the dataset type you want to use in next steps.
  type: single
  answers:
    - id: { code: TabularDatasetType }
      text: Tabular Dataset
      description: A dataset in a structured format, typically organized in rows and columns, where each column represents a feature and each row represents an instance.
    - id: { code: ImageDatasetType }
      text: Image Dataset
      description: A dataset composed also of images, where each image is associated with a label or category.
  enabled_by: []
  action_needed: null
  created_at: 2024-05-31T18:20:21

- id: { code: TabularDatasetSelection }
  text: Choose a dataset or load your own.
  description: Choose or provide a dataset which will be subject to the fairness process.
  type: single
  answers:
    - id: { code: CustomDataset }
      text: Custom
      description: Load your own dataset.
    - id: { code: AdultDataset }
      text: Adult Census Income Dataset
      description: A dataset containing demographic information used to predict whether an individual's income exceeds $50,000 per year based on attributes like age, education, and occupation.
    - id: { code: CompasDataset }
      text: ProPublica COMPAS Dataset
      description: A dataset used to assess the risk of recidivism, containing data from the COMPAS system, which is used in the U.S. criminal justice system to predict the likelihood of re-offending.
    - id: { code: CreditDataset }
      text: German Credit Dataset
      description: A dataset used for credit scoring, containing data about individuals' credit history and personal information to predict whether a person is a good or bad credit risk.
    - id: { code: UllDataset }
      text: ULL Dataset
      description: to fill
  enabled_by:
    - { code: TabularDatasetType }
  action_needed: null
  created_at: 2024-05-31T18:20:22

- id: { code: ImageDatasetSelection }
  text: Choose a dataset or load your own.
  description: Choose or provide a dataset which will be subject to the fairness process.
  type: single
  answers:
    - id: { code: CustomDataset }
      text: Custom
      description: Load your own dataset.
    - id: { code: SkinDiseaseDataset }
      text: Skin Disease Dataset
      description: A dataset used for skin disease classification, containing images of various skin conditions to assist in diagnosis.
  enabled_by:
    - { code: ImageDatasetType }
  action_needed: null
  created_at: 2024-05-31T18:20:23

- id: { code: DatasetConfirmation }
  text: Do you want to proceed with the selected dataset?
  description: null
  type: single
  answers:
  - id: { code: DatasetConfirmation-Yes }
    text: "Yes"
    description: null
  - id: { code: DatasetConfirmation-No }
    text: "No"
    description: null
  enabled_by:
    - { code: CustomDataset }
    - { code: AdultDataset }
    - { code: CompasDataset }
    - { code: CreditDataset }
    - { code: UllDataset }
    - { code: SkinDiseaseDataset }
  action_needed: null
  created_at: 2024-05-31T18:20:24

- id: { code: FeaturesSelection }
  text: | 
    Which are the sensitive and output features?
    Select features on the left. Columns display all statistical information about the features. On the right, choose the sensitive features and a target feature to predict. Features potentially sensitive based on their names are preselected automatically but you can deselect them. 
  description: Mark the features that are sensitive and the ones that are the output of the model.
  type: multiple
  answers:
    - id: { code: FeaturesSelection-features-selected }
      text: ""
      description: null
  enabled_by:
    - { code: DatasetConfirmation-Yes }
  action_needed: null
  created_at: 2024-05-31T18:20:25

- id: { code: Proxies }
  text: Specify the proxies for the sensitive features.
  description: Proxy features refer to attributes or variables in a dataset that are not explicitly discriminatory but are closely correlated with sensitive attributes like race, gender, age, or disability. These proxy features can inadvertently lead to biased decisions when used in machine learning models.
  type: multiple
  answers:
    - id: { code: Proxies-features-selected }
      text: ""
      description: null
  enabled_by:
    - { code: FeaturesSelection-features-selected }
  action_needed: null
  created_at: 2024-05-31T18:20:26

- id: { code: Detection }
  text: Select the fairness metrics and the features to check. The selected metrics will be mitigated in the next steps.
  description: Fairness metrics are quantitative tools used to assess the fairness of AI systems, ensuring all individuals or groups are treated equitably. These metrics help pinpoint biases that may arise from training data, algorithm design, or unintended model outcomes. The framework recommends the most suitable metrics based on the context analysis. For a detailed understanding of each metric's meaning and formulation, please see https://aequitas-home.readthedocs.io/en/latest/detection.html#definitions-based-on-predicted-outcome.
  type: multiple
  answers:
    - id: { code: StatisticalParityDifference }
      text: Statistical Parity Difference
      description: Statistical Parity Difference measures fairness by comparing the difference in favorable outcomes between groups. A value close to 0 indicates minimal bias.
    - id: { code: DisparateImpact }
      text: Disparate Impact
      description: Disparate Impact measures fairness by comparing favorable outcomes between groups. A ratio below 0.8 may indicate discrimination.
  enabled_by:
    - { code: Proxies-features-selected }
  action_needed: null
  created_at: 2024-05-31T18:20:27

- id: { code: DataMitigation }
  text: Which data mitigation technique do you want to apply?
  description: Data mitigation involves reducing biases in data to prevent unfair outcomes. This operation will produce a new dataset that will be used to train the model.
  type: single
  answers:
    - id: { code: DisparateImpactRemover }
      text: Disparate Impact Remover
      description: Disparate Impact Remover adjusts the model to reduce bias by minimizing disparate impacts between groups.
    - id: { code: LearnedFairRepresentations }
      text: Learned Fair Representations
      description: Learned Fair Representations transforms data to minimize bias while preserving predictive power, ensuring fairness in model outcomes.
    - id: { code: Reweighing }
      text: Reweighing
      description: Reweighing adjusts the importance of samples to correct for bias, giving more weight to underrepresented groups to promote fairness.
    - id: { code: CorrelationRemover }
      text: Correlation Remover
      description: Correlation Remover removes features highly correlated with others to reduce multicollinearity and improve model fairness.
    - id: { code: NoDataMitigation }
      text: Do Not Mitigate
      description: null
    - id: { code: Done }
      text: Do Not Mitigate and Finish
      description: null
  enabled_by:
    - { code: StatisticalParityDifference }
    - { code: DisparateImpact } # note the comeback from the summary
  action_needed: null
  created_at: 2024-05-31T18:20:28

- id: { code: DataMitigationSummary }
  text: What do you want to do next?
  description: Summary of data mitigation performed with results and updated dataset.
  type: single
  answers:
    - id: { code: MitigateDataAgain}
      text: Mitigate Data Again
      description: null
    - id: { code: MitigateModel }
      text: Mitigate Model
      description: null
    - id: { code: Done }
      text: Done
      description: null
    - id: { code: Test }
      text: Validate on Test Data
      description: null
  enabled_by:
    - { code: DisparateImpactRemover }
    - { code: LearnedFairRepresentations }
    - { code: Reweighing }
    - { code: CorrelationRemover }
  action_needed: null
  created_at: 2024-05-31T18:20:29

- id: { code: ModelMitigation }
  text: Which model mitigation technique do you want to apply?
  description: Model mitigation consists of creating a model using techniques aimed at reducing bias.
  type: single
  answers:
    - id: { code: FaUCI }
      text: FaUCI
      description: FaUCI introduces a fairness regularization approach that adjusts the model's decision boundary based on the fairness metrics.
    - id: { code: PrejudiceRemover }
      text: Prejudice Remover
      description: Prejudice Remover alters the model to reduce bias, ensuring that sensitive attributes do not influence predictions.
    - id: { code: AdversarialDebiasing }
      text: Adversarial Debiasing
      description: Adversarial Debiasing uses adversarial training to minimize bias in model predictions, ensuring fairness across sensitive groups.
    - id: { code: NoModelMitigation }
      text: Do Not Mitigate
      description: null
    - id: { code: Done }
      text: Do Not Mitigate and Finish
      description: null
  enabled_by:
    - { code: MitigateModel }
    - { code: MitigateModelAgain }
    - { code: NoDataMitigation } # note the comeback from the summary
  action_needed: null
  created_at: 2024-05-31T18:20:30

- id: { code: ModelMitigationSummary }
  text: What do you want to do next?
  description: Summary of model mitigation performed.
  type: single
  answers:
    - id: { code: MitigateDataAgain }
      text: Mitigate Data Again
      description: null
    - id: { code: MitigateModelAgain }
      text: Mitigate Model Again
      description: null
    - id: { code: MitigateOutcome }
      text: Mitigate Outcome
      description: null
    - id: { code: Test }
      text: Validate on Test Data
      description: null
    - id: { code: Done }
      text: Done
      description: null
  enabled_by:
    - { code: FaUCI }
    - { code: PrejudiceRemover }
    - { code: AdversarialDebiasing }
  action_needed: null
  created_at: 2024-05-31T18:20:31

- id: { code: OutcomeMitigation }
  text: Which outcome mitigation technique do you want to apply?
  description: Outcome mitigation involves modifying the model's predictions to ensure fairness.
  type: single
  answers:
    - id: { code: EqualizedOdds }
      text: Equalized Odds
      description: Equalized Odds is a post-processing algorithm designed to correct bias in the results of a machine learning model, ensuring that false positive rates (FPR) and false negative rates (FNR) are equal among different sensitive groups (e.g., groups based on race, gender, ethnicity). The main goal is to achieve fairness in the results predicted by the model, regardless of membership in a specific group.
    - id: { code: CalibratedEqualizedOdds }
      text: Calibrated Equalized Odds
      description: The Calibrated Equalized Odds algorithm is very similar to the Equalized Odds algorithm by adding a calibration component. It therefore respects 2 key concepts. Equalized Odds, the distribution of errors (false positives and false negatives) should be equal for all groups; Calibration, the predicted probabilities should correctly reflect the reality for each group.
    - id: { code: RejectOptionClassification }
      text: Reject Option Classification
      description: Reject Option Classification (ROC) is a post-processing algorithm used to improve fairness in machine learning models by intervening in areas where the model is less confident in its predictions.
    - id: { code: Test }
      text: Do Not Mitigate and Test
      description: null
    - id: { code: Done }
      text: Do Not Mitigate and Finish
      description: null
  enabled_by:
    - { code: MitigateOutcome }
    - { code: MitigateOutcomeAgain }
    - { code: NoModelMitigation }
  action_needed: null
  created_at: 2024-05-31T18:20:32

- id: { code: OutcomeMitigationSummary }
  text: What do you want to do next?
  description: Summary of outcome mitigation performed.
  type: single
  answers:
    - id: { code: MitigateDataAgain}
      text: Mitigate Data Again
      description: null
    - id: { code: MitigateModelAgain }
      text: Mitigate Model Again
      description: null
    - id: { code: MitigateOutcomeAgain }
      text: Mitigate Outcome Again
      description: null
    - id: { code: Test }
      text: Validate on Test Data
      description: null
    - id: { code: Done }
      text: Done
      description: null
  enabled_by:
    - { code: CalibratedEqualizedOdds }
    - { code: EqualizedOdds }
    - { code: RejectOptionClassification }
  action_needed: null
  created_at: 2024-05-31T18:20:33

- id: { code: TestSetChoice }
  text: Choose the Test Data according to your Training Data.
  description: null
  type: single
  answers:
    - id: { code: Test-CustomDataset }
      text: Custom
      description: Load your own test dataset.
    - id: { code: Test-AdultDataset }
      text: Adult Census Income Dataset
      description: Test dataset for the Adult Census Income Dataset.
    - id: { code: Test-CompasDataset }
      text: ProPublica COMPAS Dataset
      description: Test dataset for the ProPublica COMPAS Dataset.
    - id: { code: Test-CreditDataset }
      text: German Credit Dataset
      description: Test dataset for the German Credit Dataset.
    - id: { code: Test-GenerateDataset }
      text: Synthetic Data
      description: A test dataset generated with the Synthetic Data Generator.
  enabled_by:
    - { code: Test }
    - { code: TestAgain }
  action_needed: null
  created_at: 2024-05-31T18:20:34

- id: { code: Polarization }
  text: Select the polarization algorithm according to your data.
  description: null
  type: multiple
  answers:
    - id: { code: CategoricalPolarization }
      text: Categorical Polarization
      description: null
    - id: { code: NumericalPolarization }
      text: Numerical Polarization
      description: null
  enabled_by:
    - { code: Test-GenerateDataset }
  action_needed: null
  created_at: 2024-05-31T18:20:35

- id: { code: TestSummary }
  text: What do you want to do next?
  description: null
  type: single
  answers:
    - id: { code: MitigateDataAgain }
      text: Mitigate Data Again
      description: null
    - id: { code: MitigateModelAgain }
      text: Mitigate Model Again
      description: null
    - id: { code: MitigateOutcomeAgain }
      text: Mitigate Outcome Again
      description: null
    - id: { code: TestAgain }
      text: Validate on Test Data Again
      description: null
    - id: { code: Done }
      text: Done
      description: null
  enabled_by:
    - { code: Test-CustomDataset }
    - { code: Test-AdultDataset }
    - { code: Test-CompasDataset }
    - { code: Test-CreditDataset }
    - { code: CategoricalPolarization }
    - { code: NumericalPolarization }
  action_needed: null
  created_at: 2024-05-31T18:20:36

- id: { code: QuestionnaireEnd }
  text: ""
  description: null
  type: single
  answers: []
  enabled_by:
    - { code: Done }
  action_needed: null
  created_at: 2024-05-31T18:20:37